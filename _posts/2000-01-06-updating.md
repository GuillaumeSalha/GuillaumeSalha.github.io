---
title: "Set of movies"
bg: bg5
color: white
fa-icon: cloud-upload
---

# Which set of movies do we have to send to the aliens to accurately represent the Earth?

**Summary of the strategy**

From now on, we leave the point of view of aliens, and adopt the one of scientists on Earth who aim at elaborating the optimal movie dataset to send to space, so as to represent the Earth accurately. How to find the best pool? And how many movies shoud this pool contain?
To answer these questions, we begin with defining a metric that assesses scores of pools of movies. We then optimize the number *N_opt* that should be sent. We can now iteratively construct our optimal pool. This enables us to analyze the sensitivity of our scoring method. We also use machine learning methods such as linear regressions and random forests to assess the influences of movies genres on the scores.


* Definition of the metric

We first define a metric that evaluates the score of pools of movies. This score of a pool is subdivided into 4 distinct subscores, that evaluate the fit of the pool to references in the USA: a score on parity (men to women ratio); a score on distribution of heights; a score on distribution of ages; and a score on diversity of ethnicities.
Description of each metric:
1.	For the parity score, we want to emphasize the fact that pools of movies that have a women (or men) ratio under 1/3 or over 2/3 should be strongly penalized. Indeed, parity an critical characteristic of human society. If that proportion gets close to 50%, the score rises rapidly to values close to 1. Also, the score should be symmetric in men and women, i.e. a pool with a proportion *p* of females should get the same score as a pool with proportion *1-p*.
2.	For the diversity score, we wish to attribute great scores to pools of movies that collectively represent as many ethnicities as possible. If the number of different ethnicities, *N_eth*, is at least 0.75* the number of actors, *N_act*, a score of 1 is attributed. The score then decreases with this ratio. This way, we value all ethnicities the same way, and value a form of diversity that is easy to measure.
3. For the height score, we observe the reference distribution for the United States, and compare our distribution over the actors to the reference one. We choose to penalize equally all misfits. We are likely to observe pools of movies that underrepresent either old or young people. Consequently, a pool that would not underrepresent them too much are way more accurate as regards total population, and deserve good scores.
4. For the age score, we apply the same strategy. Similarly, we choose to penalize equally all misfits. The same conclusions can be drawn.


Let's have a quick look at the following scatterplot matrix. It shows the distributions and relationships between the four subscores. What we see is that each distribution is close to a gaussian function, which is good in order to find some aexisting pools significantly better than the average. Also, the very low values for $R^2$ reveal that the subscores have relatively low dependencies between them. There might be unmastered confonders, but their eventual effects seem weak. The only two scores that appear to have a correlation are parity and height. Though it is easy to associate it with the fact that more men usually leads to a higher average height among the actors, that should not be the explanation. Indeed, we have already taken this fact into consideration by separating between men and women in the heights distributions. Then, the correlation is hard to explain.

![alt text](..//img//png//tradeoff.png "Tradeoff")

<iframe src="img/png/scatterplot.png" width="900" height="600" frameborder="0" style="border: 0px"></iframe>


Insert scatterplot Matrix

* Why do we assess the scores of pools of movies and not of movies?

A movie alone may have not enough actors to be representative of society. It also may be about a particular event, place, era, that does not describe comprehensively our society (wars under the Roman Empire). It may even be about science-fiction. For these reasons, we have concluded that scores over one movie are likely to be very insignificant. 
This traduces into a fairly high variance when we look at the distribution of scores over pools of one movie only. 

<iframe src="img/png/one_movie.png" width="900" height="600" frameborder="0" style="border: 0px"></iframe>

* Then, is there an optimal number of movies per pool, 
*N_opt*?

The following **tradeoff** occurs: if N increases, the variance of the scores of K pools of N movies should decrease. But on average, the film industry is not representative of society, with obvious biases in gender and ethnicities. Therefore, an increase in N should result in increases in the bias.

<iframe src="img/png/tradeoff.png" width="900" height="600" frameborder="0" style="border: 0px"></iframe>

The picture above reveals the tradeoff. The red curve (the average spread in amplitude of our score) decreases with N. We would like it to be large, to discriminate effectively for great pools. The blue curve (inverse of variability of the mean score of 1000 pools) increases with N. We want it to be high, so that the variablility remains low. As a consequence, we choose a value of N that is not too detrimental to either of the criteria we saw.

We choose N = 20 movies per pool as of now. We can notice that the average number of actors per movie is around 8. This means that for 20 movies, we will do statistics over about 160 actors. This seems sufficient to be somehow representative. 


* How to find the optimal pool of 20 movies?


The strategy here is not to find the $\binom{N_{movies}}{20}$ best scores. This would be disproportionately complex. We rather apply the following idea. First, we create 10,000 random pools of 10 films, and keep the best 10. Thanks to this, we can have scores much higher tha, what we had with 20 movies (scores around 85 vs 75 before). A t-test reveals that such scores are critically unlikely if we compare with random distributions of pools of 20 movies. From this basis, we iterate over all remaining movies to find the one that most improves the total score. We then have 11 movies, and redo the exact same thing, until the pool is composed of 20 movies.

On the way to a bigger pool? We chose to use 10 movies with a high score to begin, because in the other case, the computational cost is too high. Interestingly, the goal of our algorithm is more to keep the excellent score of our initial pool, rather than improving it. 
Since the 10 additional movies are here to maintain the diversity of the pool, they are chosen to be the best in their respective category, so we can hope to get a mega dataset by taking these representativeness-keeping movies, and amazingly (also manually...) we obtained a pool of 60 movies having a score of 82 !
This is quite exciting, because normally, pools of that size have a maximal representativeness score that is around 50. But still, this has a severe computational cost.

* Is our model sensitive to changes in the scoring function?

A very arbitrary parameter we chose is the list of weightings that we attribute to each subscore, to sum up to the total score. What happens if this weighting is modified? Does an optimal pool for a previous scoring remain good? For this purpose, we define 6 different weightings, and evaluate the scores of these 10 pools on each new weighting. 

**Results**:

We observe that each pool has worse scores for all weightings other than the initial one. But, the scores remain in the range of excellent scores. So they are no longer the optimal pool. This traduces the fact that our metric not too one-sided but rather takes every subscore into consideration.

* **Machine learning tools**:

The goal is to assess the influence of some genres on the score of pools of movies. We decide to focus on the genres that appear in at leas 10% of the movies of our filtered dataset. This lets us with mostly Drama, Comedy, Thriller, Action Adventure, and other broad categories (10 genres in total). 
We simulate 10,000 random pools of 20 movies. For each, we build features that are equal to the frequency of each of these genres in the pool ($frequency = \frac{N_{appearances}}{20}$). We also give 5 features that should help the models fit the scores: mean and standard deviations of actors' ages and heights in the pool, and parity in the pool. 

We separate our 10,000 pools into 8,000 pools for training and 2,000 for testing. 
In order to assess whether genres influence scores or not, we perform analyses thanks to linear regression and random forests both with all the features, and then without the genres.
The following graphs show the results for the random forests model with, and without the genres.
    
The difference in standard deviation of the distributions of predicted scores between both seems negligible: 14.08 (with genres) vs 14.63 (without). A t-test also gives a very small difference in their average values, bu 
and large p value --> difference is insignificant. So for now, we conclude that the genres we have studied do not influence the accuracy in predicting scores.

<iframe src="img/html/random_forests_with.html" width="900" height="600" frameborder="0" style="border: 0px"></iframe>

<iframe src="img/html/random_forests_without.html" width="900" height="600" frameborder="0" style="border: 0px"></iframe>

Point 3. Linear regression and random forests methods applied on X1 (with the following features), and X2 (same without genres of movies), show the following results. In both cases, the prediction is quite accurate with respect to the test set (standard deviation respectively equal to.. and … for scores from 0 to 100). Now, is there any improvement enabled by the addition of the genres of the movies? To answer this question, we perform a T-test. Are results predicted by the model (random forests or linear regression) significantly better with the genres than without? It turns out that the difference is not significant, with p values of 0.54 and … . This confirms the slight gap between mean square errors of the models to the test set. The conclusion of this analysis is that knowing the main genres of movies in a pool do not hep predict its score.
